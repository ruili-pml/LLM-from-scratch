{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KV-caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# hyper‑parameters\n",
    "# ----------------------------------------------------------------------\n",
    "B           = 1        # batch size\n",
    "C           = 12       # input/channel size per token\n",
    "head_size   = 16       # hidden size of our (single) attention head\n",
    "steps       = 10       # how many new tokens to append\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# projection matrices (random for demo)\n",
    "# ----------------------------------------------------------------------\n",
    "k_proj  = torch.randn(head_size, C)\n",
    "q_proj  = torch.randn(head_size, C)\n",
    "v_proj  = torch.randn(head_size, C)\n",
    "next_proj = torch.randn(C, head_size)   # turns last hidden → next x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# seed context: one initial token vector  (B, 1, C)\n",
    "# ----------------------------------------------------------------------\n",
    "torch.manual_seed(42)\n",
    "x_seq_no_cache = torch.randn(B, 1, C)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# naïve autoregressive loop (no KV cache)\n",
    "# ----------------------------------------------------------------------\n",
    "for t in range(steps):\n",
    "    # 1) project **all** tokens seen so far\n",
    "    k = x_seq_no_cache @ k_proj.T    # (B, T, hs)\n",
    "    q = x_seq_no_cache @ q_proj.T    # (B, T, hs)    \n",
    "    v = x_seq_no_cache @ v_proj.T    # (B, T, hs)\n",
    "\n",
    "    # 2) causal attention over the full T×T matrix\n",
    "    T = k.size(1)\n",
    "    attn = q @ k.transpose(-2, -1) / math.sqrt(head_size)  # (B, T, T)\n",
    "    mask = torch.tril(torch.ones(T, T))\n",
    "    attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "    # 3) hidden state of the **last** position\n",
    "    out = attn @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "    out_last = out[:, -1, :]    # (B, hs)\n",
    "\n",
    "    # 4) predict next token\n",
    "    x_next = out_last @ next_proj.T # (B, hs) @ (hs, C) -> (B, C)\n",
    "    x_next = x_next.unsqueeze(1) # (B, C) -> (B, 1 ,C)\n",
    "\n",
    "    # 5) append to sequence\n",
    "    x_seq_no_cache = torch.cat([x_seq_no_cache, x_next], dim=1) # (B, T, C) (B, 1, C) -> (B, T+1, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# seed context and empty KV cache\n",
    "# ----------------------------------------------------------------------\n",
    "torch.manual_seed(42)\n",
    "x_seq_kv_cache = torch.randn(B, 1, C)          # (B, 1, C)  initial token\n",
    "k_cache = torch.zeros(B, 0, head_size)  # (B, 0, hs), tensor([])\n",
    "v_cache = torch.zeros(B, 0, head_size)  # (B, 0, hs), tensor([])\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# KV‑cached autoregressive loop\n",
    "# ----------------------------------------------------------------------\n",
    "for _ in range(steps):\n",
    "    \n",
    "    # 1) project **current** token only\n",
    "    x_step = x_seq_kv_cache[:, -1, :].unsqueeze(1)  # current input token (B,1,C)\n",
    "\n",
    "    k_step = x_step @ k_proj.T     # (B, 1, hs)\n",
    "    q_step = x_step @ q_proj.T     # (B, 1, hs)\n",
    "    v_step = x_step @ v_proj.T     # (B, 1, hs)\n",
    "\n",
    "    # 2) append new key/value to the cache\n",
    "    k_cache = torch.cat([k_cache, k_step], dim=1)  # (B, t, hs)\n",
    "    v_cache = torch.cat([v_cache, v_step], dim=1)  # (B, t, hs)\n",
    "\n",
    "    # 3) causal attention over cached keys (size 1 × (t+1))\n",
    "    attn   = q_step @ k_cache.transpose(-2, -1) / math.sqrt(head_size)\n",
    "        # (B, 1, hs) @ (B, hs, t) -> (B, 1, t)\n",
    "    attn   = F.softmax(attn, dim=-1)              # (B,1,t)\n",
    "\n",
    "    # 4) hidden state for current position\n",
    "    out_last = attn @ v_cache\n",
    "        # (B, 1, t) @ (B, t, hs) -> (B, 1, hs)\n",
    "\n",
    "    # 5) predict next token vector\n",
    "    x_next = out_last @ next_proj.T\n",
    "        # (B, 1, hs) @ (hs, C) -> (B, 1, C)\n",
    "\n",
    "    # 6) append next token to full sequence & advance\n",
    "    x_seq_kv_cache = torch.cat([x_seq_kv_cache, x_next], dim=1)     # (B, t+1, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(x_seq_no_cache, x_seq_kv_cache)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
