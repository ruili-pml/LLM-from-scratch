{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da72a67-a866-4685-b120-72b9a8ca136b",
   "metadata": {},
   "source": [
    "# Unicode and UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f156e7-fc56-4558-95be-f8b8198b1896",
   "metadata": {},
   "source": [
    "## Unicode\n",
    "\n",
    "We can get Unicode representation by \n",
    "```\n",
    "ord(char)  # character to unicode, returned number is in decimal\n",
    "\n",
    "chr(unicode)  # unicode to character\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "895f5ca2-7c05-4833-9c59-07eb5f2c3cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc5d1359-e4a9-413b-9716-41f5dd8fa1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(97)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1386ec26-5f82-44e5-8d1e-a82c24a96ec1",
   "metadata": {},
   "source": [
    "## UTF-8\n",
    "\n",
    "We can get UTF-8 representation by\n",
    "\n",
    "```\n",
    "char/string.encode(\"utf-8\")  # character or string to UTF-8. default return in hex\n",
    "\n",
    "uft8.decode(\"utf-8\") # UTF-8 to character or string\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842a103c-a485-46a5-baa2-4e5e254f563e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xc3\\xa9'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ã©\".encode(\"utf-8\")  # raw byte representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d7e772-6b0b-4b27-891f-56332764ea87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ã©'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_byte = \"Ã©\".encode(\"utf-8\")\n",
    "raw_byte.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4faf9b3b-ab44-4fc1-a6db-35a03c26594e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[195, 169]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list can convert raw byte representation into readable decimal number\n",
    "list(\"Ã©\".encode(\"utf-8\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a6cd54-7f89-4b09-b53d-581b8ac0caa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ã©'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode list number back into raw byte\n",
    "bytes([195, 169]).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f3278-483b-41d5-aa80-8b8226f284b2",
   "metadata": {},
   "source": [
    "## Compare two encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a6917f8-c3e9-4805-a9a8-43b5dc7489ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n",
      "---\n",
      "\n",
      "length of character / Unicode code point: 533\n",
      "---\n",
      "\n",
      "length of raw bytes: 616\n"
     ]
    }
   ],
   "source": [
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(text)\n",
    "print('---')\n",
    "print(\"\\nlength of character / Unicode code point:\", len(text))\n",
    "print('---')\n",
    "print(\"\\nlength of raw bytes:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bece37-b004-455f-98a9-25ea2f7ec37f",
   "metadata": {},
   "source": [
    "# Byte-pairing Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51f28f97-9d1f-4737-9e45-99fdca32eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(tokenised_text):\n",
    "    \"\"\"\n",
    "    Count the pair appearing time\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    for pair in zip(tokenised_text, tokenised_text[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def tokenised_input(old_tokenised_text, pair, new_token_idx):\n",
    "    \"\"\"\n",
    "    Use the new vocabulary to tokenise input. \n",
    "    Equivalent to merge the pair into the new token idx.\n",
    "    \n",
    "    Args:\n",
    "    old_tokenised_text: [189, 142, 239, 189, 1, ...]  # tokenised text based on old vocab\n",
    "    pair: (idx_1, idx_2), most common pair\n",
    "    new_token_idx: index for new token in the vocabulary\n",
    "    Returns:\n",
    "    Inputs tokenised by new vocabulary\n",
    "    \"\"\" \n",
    "    new_tokenised_text = []\n",
    "    i = 0\n",
    "    while i < len(old_tokenised_text):\n",
    "        if i < len(old_tokenised_text) - 1 and old_tokenised_text[i] == pair[0] and old_tokenised_text[i+1] == pair[1]:\n",
    "            new_tokenised_text.append(new_token_idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokenised_text.append(old_tokenised_text[i])\n",
    "            i += 1\n",
    "    return new_tokenised_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b37a2f3-a782-44d9-9850-cdaf414296d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (240, 159) into a new token 257\n",
      "merging (226, 128) into a new token 258\n",
      "merging (105, 110) into a new token 259\n",
      "merging (115, 32) into a new token 260\n",
      "merging (97, 110) into a new token 261\n",
      "merging (116, 104) into a new token 262\n",
      "merging (257, 133) into a new token 263\n",
      "merging (257, 135) into a new token 264\n",
      "merging (97, 114) into a new token 265\n",
      "merging (239, 189) into a new token 266\n",
      "merging (258, 140) into a new token 267\n",
      "merging (267, 264) into a new token 268\n",
      "merging (101, 114) into a new token 269\n",
      "merging (111, 114) into a new token 270\n",
      "merging (116, 32) into a new token 271\n",
      "merging (259, 103) into a new token 272\n",
      "merging (115, 116) into a new token 273\n",
      "merging (261, 100) into a new token 274\n",
      "merging (32, 262) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 276 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "tokenised_text = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(tokenised_text)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"merging {pair} into a new token {idx}\")\n",
    "  tokenised_text = tokenised_input(tokenised_text, pair, idx)\n",
    "  merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ce2828c-49bf-4f97-87a2-5aa5e8cd2b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw bytes length: 616\n",
      "encoded token size: 451\n",
      "compression ratio: 1.37X\n"
     ]
    }
   ],
   "source": [
    "print(\"raw bytes length:\", len(tokens))\n",
    "print(\"encoded token size:\", len(tokenised_text))\n",
    "print(f\"compression ratio: {len(tokens) / len(tokenised_text):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6211fa3c-5713-4f93-a718-d84bb5df34cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python generic (scicomp-python-env/2024-01)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
