# LLM-from-scratch

## Table of Contents

### <1> tokenisation
- Unicode, UTF-8
- Byte-pair Encoding 
- Problems caused by tokenisation

### <2> vanilla transformer
transformer.md
- self-attention
- multi-head attention
- common transformer structure
- training details (gradient clipping, learning rate schedule, gradeint accumulation)

computation.md
- mixed precision

## references

https://www.youtube.com/watch?v=zduSFxRajkE

https://www.youtube.com/watch?v=l8pRSuU81PU

https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=140s

https://stanford-cs336.github.io/spring2025/